/*
 * Model Server
 *
 * This is API spec of model server
 *
 * API version: 0.0.1
 * Contact: hello@instill.tech
 * Generated by: OpenAPI Generator (https://openapi-generator.tech)
 */

package handlers

import (
	"archive/zip"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"os"
	"path"
	"path/filepath"
	"regexp"
	"strconv"
	"strings"
	"time"

	"github.com/google/uuid"

	"github.com/pkg/errors"
	"google.golang.org/grpc/codes"
	"google.golang.org/grpc/status"
	"google.golang.org/protobuf/types/known/anypb"
	"google.golang.org/protobuf/types/known/timestamppb"

	"github.com/instill-ai/model-backend/configs"
	"github.com/instill-ai/model-backend/internal-protogen-go/model"
	"github.com/instill-ai/model-backend/internal/triton"
	"github.com/instill-ai/model-backend/pkg/db"
	"github.com/instill-ai/model-backend/pkg/models"
)

type ServiceHandlers struct{}

func _isTritonServerReady() bool {
	serverLiveResponse := triton.ServerLiveRequest(triton.TritonClient)
	if serverLiveResponse == nil {
		return false
	}
	fmt.Printf("Triton Health - Live: %v\n", serverLiveResponse.Live)
	if !serverLiveResponse.Live {
		return false
	}

	serverReadyResponse := triton.ServerReadyRequest(triton.TritonClient)
	fmt.Printf("Triton Health - Ready: %v\n", serverReadyResponse.Ready)
	return serverReadyResponse.Ready
}

func _createModelResponse(modelInDB models.Model, versions []models.Version) *model.CreateModelResponse {
	var mRes model.CreateModelResponse
	mRes.Name = modelInDB.Name
	mRes.Id = modelInDB.Id
	mRes.Optimized = modelInDB.Optimized
	mRes.Description = modelInDB.Description
	mRes.Framework = modelInDB.Framework
	mRes.CreatedAt = &model.Timestamp{Timestamp: timestamppb.New(modelInDB.CreatedAt)}
	mRes.UpdatedAt = &model.Timestamp{Timestamp: timestamppb.New(modelInDB.UpdatedAt)}
	mRes.Organization = modelInDB.Organization
	mRes.Icon = modelInDB.Icon
	mRes.Type = modelInDB.Type
	mRes.Visibility = modelInDB.Visibility
	var vers []*model.ModelVersion
	for i := 0; i < len(versions); i++ {
		vers = append(vers, &model.ModelVersion{
			Version:     versions[i].Version,
			ModelId:     versions[i].ModelId,
			Description: versions[i].Description,
			CreatedAt:   &model.Timestamp{Timestamp: timestamppb.New(versions[i].CreatedAt)},
			UpdatedAt:   &model.Timestamp{Timestamp: timestamppb.New(versions[i].UpdatedAt)},
		})
	}
	mRes.Versions = vers
	return &mRes
}

//writeToFp takes in a file pointer and byte array and writes the byte array into the file
//returns error if pointer is nil or error in writing to file
func _writeToFp(fp *os.File, data []byte) error {
	w := 0
	n := len(data)
	for {

		nw, err := fp.Write(data[w:])
		if err != nil {
			return err
		}
		w += nw
		if nw >= n {
			return nil
		}
	}
}

func _unzip(filePath string, dstDir string) ([]*models.Model, []*models.Version, bool) {
	archive, err := zip.OpenReader(filePath)
	if err != nil {
		fmt.Println("Error when open zip file ", err)
		return []*models.Model{}, []*models.Version{}, false
	}
	defer archive.Close()

	var createdModels []*models.Model
	var createdVersions []*models.Version
	var currentModelName string
	for _, f := range archive.File {
		if strings.Contains(f.Name, "__MACOSX") { // ignore temp directory in macos
			continue
		}
		filePath := filepath.Join(dstDir, f.Name)
		fmt.Println("unzipping file ", filePath)

		if !strings.HasPrefix(filePath, filepath.Clean(dstDir)+string(os.PathSeparator)) {
			fmt.Println("invalid file path")
			return []*models.Model{}, []*models.Version{}, false
		}
		if f.FileInfo().IsDir() {
			dirName := f.Name
			if string(dirName[len(dirName)-1]) == "/" {
				dirName = dirName[:len(dirName)-1]
			}
			if !strings.Contains(dirName, "/") { // top directory model
				currentModelName = dirName
				newModel := &models.Model{
					Name:         dirName,
					Id:           dirName,
					CreatedAt:    time.Now(),
					UpdatedAt:    time.Now(),
					Type:         "tensorrt",
					Framework:    "pytorch",
					Optimized:    false,
					Icon:         "",
					Visibility:   "public",
					Organization: "domain@instill.tech",
					Author:       "local-user@instill.tech",
				}
				createdModels = append(createdModels, newModel)
			} else { // version folder
				patternVersionFolder := fmt.Sprintf("^%v/[0-9]+$", currentModelName)
				match, _ := regexp.MatchString(patternVersionFolder, dirName)
				if match {
					elems := strings.Split(dirName, "/")
					sVersion := elems[len(elems)-1]
					iVersion, err := strconv.Atoi(sVersion)
					if err == nil {
						newVersion := &models.Version{
							Version:   int32(iVersion),
							ModelId:   currentModelName,
							CreatedAt: time.Now(),
							UpdatedAt: time.Now(),
							Status:    "offline",
							Metadata:  models.JSONB{},
						}
						createdVersions = append(createdVersions, newVersion)
					}

				}
			}

			fmt.Println("creating directory... ", filePath)
			_ = os.MkdirAll(filePath, os.ModePerm)
			continue
		}

		if err := os.MkdirAll(filepath.Dir(filePath), os.ModePerm); err != nil {
			return []*models.Model{}, []*models.Version{}, false
		}

		dstFile, err := os.OpenFile(filePath, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, f.Mode())
		if err != nil {
			return []*models.Model{}, []*models.Version{}, false
		}

		fileInArchive, err := f.Open()
		if err != nil {
			return []*models.Model{}, []*models.Version{}, false
		}

		if _, err := io.Copy(dstFile, fileInArchive); err != nil {
			return []*models.Model{}, []*models.Version{}, false
		}

		dstFile.Close()
		fileInArchive.Close()
	}
	return createdModels, createdVersions, true
}

func _saveFile(stream model.Model_CreateModelServer) (outFile string, err error) {
	firstChunk := true
	var fp *os.File

	var fileData *model.CreateModelRequest

	var tmpFile string

	for {
		fileData, err = stream.Recv() //ignoring the data  TO-Do save files received
		if err != nil {
			if err == io.EOF {
				break
			}

			err = errors.Wrapf(err,
				"failed unexpectadely while reading chunks from stream")
			return "", err
		}

		if firstChunk { //first chunk contains file name
			if fileData.Filename != "" {
				tmpFile = path.Join("/tmp", filepath.Base(fileData.Filename))
				fp, err = os.Create(tmpFile)
				if err != nil {
					return "", err
				}
				defer fp.Close()
			} else {
				return "", errors.Errorf("No filename")
			}
			firstChunk = false
		}
		err = _writeToFp(fp, fileData.Content)
		if err != nil {
			return "", err
		}
	}
	return tmpFile, nil
}

func _savePredictInput(stream model.Model_PredictModelServer) (imageFile string, modelId string, version int, modelType triton.CVTask, err error) {
	firstChunk := true
	var fp *os.File

	var fileData *model.PredictModelRequest

	var tmpFile string

	for {
		fileData, err = stream.Recv() //ignoring the data  TO-Do save files received
		if err != nil {
			if err == io.EOF {
				break
			}

			err = errors.Wrapf(err,
				"failed unexpectadely while reading chunks from stream")
			return "", "", -1, 0, err
		}

		if firstChunk { //first chunk contains file name
			modelId = fileData.Name
			version = int(fileData.Version)
			modelType = triton.CVTask(fileData.Type)

			tmpFile = path.Join("/tmp/", uuid.New().String())
			fp, err = os.Create(tmpFile)
			if err != nil {
				return "", "", -1, 0, err
			}
			defer fp.Close()

			firstChunk = false
		}
		err = _writeToFp(fp, fileData.Content)
		if err != nil {
			return "", "", -1, 0, err
		}
	}
	return tmpFile, modelId, version, modelType, nil
}

func _makeError(statusCode codes.Code, title string, detail string, duration float64) error {
	err := &models.Error{
		Status: int32(statusCode),
		Title:  title,
		Detail: detail,
	}
	data, _ := json.Marshal(err)
	return status.Error(statusCode, string(data))
}

// AddModel - upload a model to the model server
func (s *ServiceHandlers) CreateModel(stream model.Model_CreateModelServer) (err error) {
	start := time.Now()

	tmpFile, err := _saveFile(stream)
	if err != nil {
		return _makeError(400, "Save File Error", err.Error(), float64(time.Since(start).Milliseconds()))
	}

	// extract zip file from tmp to models directory
	createdModels, createdVersions, isOk := _unzip(tmpFile, configs.Config.TritonServer.ModelStore)
	if !isOk || len(createdModels) == 0 {
		return _makeError(400, "Save File Error", "Could not extract zip file", float64(time.Since(start).Milliseconds()))
	}

	var respModels = []*model.CreateModelResponse{}
	for i := 0; i < len(createdModels); i++ {
		newModel := createdModels[i]
		// check model existed or not
		var modelInDB models.Model
		var versions []models.Version
		result := db.DB.Model(&models.Model{}).Where("id", newModel.Id).First(&modelInDB)
		if result.Error == nil { // model already existed
			// check version exited or not
			for j := 0; j < len(createdVersions); j++ { // this list contain versions of all models, so need check model id; TODO: maybe use bidirection link in DB
				if createdVersions[j].ModelId != modelInDB.Id {
					continue
				}
				var newVersion models.Version
				rs := db.DB.Model(&models.Version{}).Where("version", createdVersions[j].Version).Where("model_id", modelInDB.Id).First(&newVersion)
				fmt.Println(".  res", rs)
				if rs.Error != nil { // version already existed
					newVersion = models.Version{
						Version:     createdVersions[j].Version,
						ModelId:     modelInDB.Id,
						Description: modelInDB.Description,
						CreatedAt:   time.Now(),
						UpdatedAt:   time.Now(),
						Status:      "offline",
						Metadata:    models.JSONB{},
					}
					_ = db.DB.Create(&newVersion)
				}
				versions = append(versions, newVersion)
			}
			respModel := _createModelResponse(modelInDB, versions)
			respModels = append(respModels, respModel)
			continue
		}

		result = db.DB.Create(newModel)
		if result.Error != nil {
			continue
		}

		for j := 0; j < len(createdVersions); j++ {
			if createdVersions[j].ModelId != modelInDB.Id {
				continue
			}
			var newVersion models.Version
			rs := db.DB.Model(&models.Version{}).Where("version", createdVersions[j].Version).Where("model_id", modelInDB.Id).First(&newVersion)
			if rs.Error != nil { // version already existed
				newVersion = models.Version{
					Version:     createdVersions[j].Version,
					ModelId:     modelInDB.Id,
					Description: modelInDB.Description,
					CreatedAt:   time.Now(),
					UpdatedAt:   time.Now(),
					Status:      "offline",
					Metadata:    models.JSONB{},
				}
				_ = db.DB.Create(&newVersion)
			}
			versions = append(versions, newVersion)
		}

		result = db.DB.Model(&models.Model{}).Where("id", newModel.Id).First(&modelInDB)
		if result.Error != nil {
			return _makeError(500, "Add Model Error1111", result.Error.Error(), float64(time.Since(start).Milliseconds()))
		}

		respModel := _createModelResponse(modelInDB, versions)
		respModels = append(respModels, respModel)
	}

	var res model.CreateModelsResponse
	res.Models = respModels
	err = stream.SendAndClose(&res)
	if err != nil {
		return _makeError(500, "Add Model Error2222", err.Error(), float64(time.Since(start).Milliseconds()))
	}

	return
}

func (s *ServiceHandlers) LoadModel(ctx context.Context, in *model.LoadModelRequest) (*model.LoadModelResponse, error) {
	fmt.Println("Load model ", in)

	if !_isTritonServerReady() {
		return &model.LoadModelResponse{}, nil
	}
	loadModelResponse := triton.LoadModelRequest(triton.TritonClient, in.ModelId)
	fmt.Println(loadModelResponse)

	return &model.LoadModelResponse{}, nil
}

func (s *ServiceHandlers) UnloadModel(ctx context.Context, in *model.UnloadModelRequest) (*model.UnloadModelResponse, error) {
	fmt.Println("UnloadModel model ", in)

	if !_isTritonServerReady() {
		return &model.UnloadModelResponse{}, nil
	}
	unloadModelResponse := triton.UnloadModelRequest(triton.TritonClient, in.ModelId)
	fmt.Println(unloadModelResponse)

	return &model.UnloadModelResponse{}, nil
}

func (s *ServiceHandlers) ListModels(ctx context.Context, in *model.ListModelRequest) (*model.ListModelResponse, error) {
	fmt.Println("ListModels model ", in)

	if !_isTritonServerReady() {
		return &model.ListModelResponse{}, nil
	}

	listModelsResponse := triton.ListModelsRequest(triton.TritonClient)
	fmt.Println("listModelsResponse ", listModelsResponse)

	var resModels []*model.CreateModelResponse
	models := listModelsResponse.Models
	for i := 0; i < len(models); i++ {
		md := model.CreateModelResponse{
			Id:   models[i].Name,
			Name: models[i].Name,
		}
		resModels = append(resModels, &md)
	}
	fmt.Println("resModels ", resModels)
	return &model.ListModelResponse{Models: resModels}, nil
}

func (s *ServiceHandlers) PredictModel(stream model.Model_PredictModelServer) error {
	start := time.Now()

	if !_isTritonServerReady() {
		return _makeError(500, "PredictModel", "Triton Server not ready yet", float64(time.Since(start).Milliseconds()))
	}

	imageFile, modelId, version, cvTask, err := _savePredictInput(stream)

	modelMetadataResponse := triton.ModelMetadataRequest(triton.TritonClient, modelId, fmt.Sprint(version))
	if modelMetadataResponse == nil {
		return _makeError(400, "PredictModel", "Model not found", float64(time.Since(start).Milliseconds()))
	}

	modelConfigResponse := triton.ModelConfigRequest(triton.TritonClient, modelId, fmt.Sprint(version))
	if modelMetadataResponse == nil {
		return _makeError(400, "PredictModel", "Model config not found", float64(time.Since(start).Milliseconds()))
	}

	if err != nil {
		return _makeError(500, "PredictModel", "Could not save file", float64(time.Since(start).Milliseconds()))
	}

	fmt.Println(modelMetadataResponse)

	// err = stream.SendAndClose(&model.PredictModelResponse{})

	input, err := triton.PreProcess(imageFile, modelMetadataResponse, modelConfigResponse, cvTask)
	if err != nil {
		return _makeError(400, "PredictModel", err.Error(), float64(time.Since(start).Milliseconds()))
	}

	// /* We use a simple model that takes 2 input tensors of 16 integers
	// each and returns 2 output tensors of 16 integers each. One
	// output tensor is the element-wise sum of the inputs and one
	// output is the element-wise difference. */
	inferResponse, err := triton.ModelInferRequest(triton.TritonClient, cvTask, input, modelId, fmt.Sprint(version), modelMetadataResponse, modelConfigResponse)
	if err != nil {
		return _makeError(500, "PredictModel", err.Error(), float64(time.Since(start).Milliseconds()))
	}

	// /* We expect there to be 2 results (each with batch-size 1). Walk
	// over all 16 result elements and print the sum and difference
	// calculated by the model. */
	postprocessResponse, err := triton.PostProcess(inferResponse, modelMetadataResponse, cvTask)
	if err != nil {
		return _makeError(500, "PredictModel", err.Error(), float64(time.Since(start).Milliseconds()))
	}

	switch cvTask {
	case triton.Classification:
		clsResponses := postprocessResponse.([]string)
		var contents []*model.ClassificationOutput
		for _, clsRes := range clsResponses {
			clsResSplit := strings.Split(clsRes, ":")
			if len(clsResSplit) != 3 {
				return _makeError(500, "PredictModel", "Unable to decode inference output", float64(time.Since(start).Milliseconds()))
			}
			score, err := strconv.ParseFloat(clsResSplit[0], 32)
			if err != nil {
				return _makeError(500, "PredictModel", "Unable to decode inference output", float64(time.Since(start).Milliseconds()))
			}
			clsOutput := model.ClassificationOutput{
				Category: clsResSplit[2],
				Score:    float32(score),
			}
			contents = append(contents, &clsOutput)
		}
		clsOutputs := model.ClassificationOutputs{
			Contents: contents,
		}
		data, err := anypb.New(&clsOutputs)
		if err != nil {
			return _makeError(500, "PredictModel", err.Error(), float64(time.Since(start).Milliseconds()))
		}
		err = stream.SendAndClose(&model.PredictModelResponse{Data: data})
		return err
	case triton.Detection:
		detResponses := postprocessResponse.(triton.DetectionOutput)
		batchedOutputDataBboxes := detResponses.Boxes
		batchedOutputDataLabels := detResponses.Labels
		var detOutputs model.DetectionOutputs
		for i := range batchedOutputDataBboxes {
			var contents []*model.BoundingBoxPrediction
			for j := range batchedOutputDataBboxes[i] {
				box := batchedOutputDataBboxes[i][j]
				label := batchedOutputDataLabels[i][j]

				// Non-meaningful bboxes were added with coords [-1, -1, -1, -1, -1] and label "0" for Triton to be able to batch Tensors
				if label != "0" {
					pred := &model.BoundingBoxPrediction{
						Category: label,
						Score:    box[4],
						// Convert x1y1x2y2 to xywh where xy is top-left corner
						Box: &model.Box{
							Left:   box[0],
							Top:    box[1],
							Width:  box[2] - box[0],
							Height: box[3] - box[1],
						},
					}
					contents = append(contents, pred)
				}
			}
			detOutput := &model.DetectionOutput{
				Contents: contents,
			}
			detOutputs.Contents = append(detOutputs.Contents, detOutput)
		}
		data, err := anypb.New(&detOutputs)
		if err != nil {
			return _makeError(500, "PredictModel", err.Error(), float64(time.Since(start).Milliseconds()))
		}
		err = stream.SendAndClose(&model.PredictModelResponse{Data: data})
		return err
	default:
		return _makeError(500, "PredictModel", fmt.Sprintf("modelType %v do not support", cvTask), float64(time.Since(start).Milliseconds()))
	}
}

func HandleUploadOutput(w http.ResponseWriter, r *http.Request, pathParams map[string]string) {
	contentType := r.Header.Get("Content-Type")

	if contentType == "multipart/form-data" {
	} else {
		w.Header().Add("Content-Type", "application/json+problem")
		w.WriteHeader(405)
	}
}
