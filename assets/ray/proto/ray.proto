syntax = "proto3";

package ray.serve;

option go_package = "./rayserver";

// ModelReadyRequest represents a request to check if a model is ready
message ModelReadyRequest {
  // model id
  string name = 1;
  // model tag verion
  string version = 2;
}

// ModelReadyResponse represents a response to check if a model is ready
message ModelReadyResponse {
  // whether the model is ready or not
  bool ready = 1;
}

// ModelMetadataRequest represents a request to get the model metadata
message ModelMetadataRequest {
  // model id
  string name = 1;
  // model tag verion
  string version = 2;
}

// tensor for inference
message InferTensor {
  // tensor name.
  string name = 1;
  // tensor data type.
  string datatype = 2;
  // tensor shape.
  repeated int64 shape = 3;
}

// ModelMetadataResponse represents a response to get the model metadata
message ModelMetadataResponse {
  // metadata for a tensor
  message TensorMetadata {
    // tensor name
    string name = 1;
    // tensor data type
    string datatype = 2;
    // tensor shape
    repeated int64 shape = 3;
  }
  // model name
  string name = 1;
  // model tag version
  repeated string versions = 2;
  // model inference framework
  string framework = 3;
  // model inputs
  repeated TensorMetadata inputs = 4;
  // model outputs
  repeated TensorMetadata outputs = 5;
}

// ModelInferRequest represents a request for model inference
message ModelInferRequest {
  // An output tensor requested for an inference request.
  message InferRequestedOutputTensor {
    // tensor name.
    string name = 1;
  }
  // name of the model to use for inferencing.
  string model_name = 1;
  // model tag version
  string model_version = 2;
  // input tensors for the inference.
  repeated InferTensor inputs = 3;
  // The requested output tensors for the inference. Optional, if not
  // specified all outputs specified in the model config will be
  // returned.
  repeated InferRequestedOutputTensor outputs = 6;
  // raw input contents
  repeated bytes raw_input_contents = 7;
}

// ModelInferResponse represents a response for model inference
message ModelInferResponse {
  // name of the model to use for inferencing.
  string model_name = 1;
  // model tag version
  string model_version = 2;
  // output tensors
  repeated InferTensor outputs = 5;
  // raw output contents
  repeated bytes raw_output_contents = 6;
}

// Ray service for internal process
service RayService {
  // ModelReady method receives a ModelReadyRequest message and
  // returns a ModelReadyResponse
  rpc ModelReady(ModelReadyRequest) returns (ModelReadyResponse) {}
  // ModelMetadata method receives a ModelMetadataRequest message and
  // returns a ModelMetadataResponse
  rpc ModelMetadata(ModelMetadataRequest) returns (ModelMetadataResponse) {}
  // ModelInfer method receives a ModelInferRequest message and
  // returns a ModelInferResponse
  rpc ModelInfer(ModelInferRequest) returns (ModelInferResponse) {}
}
